{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982cfd7c-30cc-4611-9efe-8c260f006d42",
   "metadata": {},
   "source": [
    "2025-10-02\n",
    "\n",
    "# 学習内容\n",
    "### 書籍/教材:スッキリわかるPythonによる機械学習入門\n",
    "### 範囲: 第11章 さまざまな教師あり学習：回帰 \n",
    "### 目的: リッジ回帰、ラッソ回帰、回帰木について学ぶ\n",
    "\n",
    " -----------------------------\n",
    "## <関数まとめ>\n",
    "・pf = PolynomialFeatures(degree =  整数,include_bias = False)\n",
    "　作成後データの変数 = pf.fit_transform(データフレーム)\n",
    "\n",
    "・pf.get_feature_names_out()  \n",
    "\n",
    "・モデル変数 = Ridge(alpha = 数値) # alphaを大きくすると過学習防止効果が強くなるが、大きくしすぎると予測性能が低下する。\n",
    "\n",
    "\n",
    " -----------------------------\n",
    "## <内容まとめ>\n",
    "\n",
    "・**リッジ回帰**  \n",
    "**リッジ回帰**は以下のような回帰式を作ることを目的とする。  \n",
    "\n",
    "```\n",
    "(興行収入) = 100×(SNS1のつぶやき数) + 120 × (SNS2のつぶやき数) - 50 \n",
    "```\n",
    "\n",
    "リッジ回帰は、係数をできるだけ小さくしつつ、予測と実際の誤差を小さくするというコンセプトの上で成り立っている。  \n",
    "\n",
    "最小二乗誤差  \n",
    "$$ \n",
    "E = e_{1}^{2} + e_{2}^{2} + e_{3}^{2} + e_{4}^{2}\n",
    "$$\n",
    "\n",
    "**正則化項**(係数の二乗の合計)  \n",
    "$$\n",
    "F = a^2 + b^2\n",
    "$$\n",
    "\n",
    "リッジ回帰は**L = E + F**として、**Lが最小になるような係数aとb**を算出する。リッジ回帰を用いることで、**過学習を防げる可能性が高くなる。**  \n",
    "\n",
    "・**バイアス・バリアンス分解**  \n",
    "過学習とは「訓練データでは予測と実際の誤差が少ないのに、未知のテストデータでは、誤差が大きくなる現象」だった。この未知データの誤差は**バイアス、バリアンス、ノイズ**の３つの要素に分解できる。そして、未知データの予測誤差は、  \n",
    "\n",
    "```\n",
    "実際と予測の誤差 = バイアス＋バリアンス＋ノイズ\n",
    "```\n",
    "\n",
    "と分解でき、これを**バイアス・バリアンス分解**と呼ぶ。 \n",
    "  \n",
    "例として、1つのデータセットからランダムサンプリングを5回行い、モデルを5個作ることを考える。回帰式は  \n",
    "(興行収入) = A × (SNS1) + B  \n",
    "とする。\n",
    "ここで、学習データは各モデルで異なるので、各モデルの回帰式の係数Aおよび定数Bも異なる。そしてランダムサンプリングで選ばれなかったデータを正解データとしたとき、同じSNS1 = 170に対する正解データが根本的にばらついていたら、どのような予測値を返したら良い精度なのか判断しようがない。そこで分布の代表値である平均値=100を各モデルが予測すべき値とする。  \n",
    "そうしたときに誤差が生じる不適切な状況を考えると、次の二通りがある。  \n",
    "〇予測結果自体は密集しているが、根本的に予測結果の平均値が100から遠く離れたところにある。(**バイアス**が高い。)  \n",
    "〇予測結果の平均は100に近いが、予測結果自体にばらつきが大きく生じている。(**バリアンス**が高い。)  \n",
    "未知データにおいて、予測と実際の誤差はこのバイアスが高い状態とバリアンスが高い状態が原因であり、実世界のデータはこの二つの要因が同時に組み合わさっている。  \n",
    "ノイズは必ず発生してしまうものでありどうしようもないため、**分析者は残りのバイアスとバリアンスを下げるようにモデルチューニングやデータ加工をする。**  \n",
    "\n",
    "・**バリアンスと過学習**  \n",
    "バイアスが高い状態は、モデルが訓練データ内の法則を十分に捉えきれてないときに起きる。そのため、重回帰分析のときは特徴量の列を増やしたり、決定木の場合は木の深さを深くする(モデルを複雑にする)ことでバイアスを下げることができる。  \n",
    "しかし、モデルを複雑にすると過学習が起きてしまう。すなわち、**過学習とは、モデルを必要以上に複雑にした結果、バリアンスが異常に高くなった状態のこと。**  \n",
    "モデルが複雑なことと、予測結果の分散が大きいことに関して以下のように考えると、モデルが複雑ならば、予測結果のばらつきが大きくなるといえそうである。  \n",
    "  \n",
    "1.各ランダムサンプリングで、データセットの選び方に偏りが生じる。  \n",
    "2.モデルが複雑ならば、偏ったデータセットに対して完璧にフィットするように学習が行われてしまう。  \n",
    "3.別のランダムサンプリングで得られたデータセットの場合でも同様に、そのデータセットにのみ完璧にフィットするように学習してしまう。そのため、同じ入力データx(未知データ)の予測結果に対して、モデルごとにばらつきが生じる。  \n",
    "  \n",
    "よって、バリアンスを低く抑えるためには次の二つのアプローチがある。　　\n",
    "**A.データの件数を増やす**  \n",
    "**B.データセットが偏っても、同じような学習結果になる分析手法を選択する。**  \n",
    "ここでリッジ回帰は、「係数が大きくなりすぎては駄目」という制限を加えた重回帰分析であると考えることができるため、回帰式の係数は同じような値になりやすい。(Bのアプローチに適している。)よってリッジ回帰はバリアンスを低く抑えるのに適しており、過学習を防ぐ可能性が高い手法であるといえる。  \n",
    "\n",
    "・**多項式項と交互作用特徴量の一括作成**\n",
    "第８章の特徴量エンジニアリングで行った、2乗列や交互作用特徴量をPolynomialFeaturesメソッドを用いることで一括で作成できる。degreeで追加する次数を指定している。\n",
    "```python\n",
    "pf = PolynomialFeatures(degree = 整数, include_bias = False)\n",
    "作成後データの変数 = pf.fit_transform(データフレーム)\n",
    "```\n",
    "列名の確認\n",
    "```python\n",
    "pf.get_feature_names_out()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "・**リッジ回帰モデルの作成**\n",
    "リッジ回帰モデルはRidge関数を使って作ることができる。alphaを大きくすると過学習防止効果が強くなるが、大きくしすぎると予測性能が低下する。\n",
    "```python\n",
    "モデル変数 = Ridge(alpha = 数値)\n",
    "```\n",
    "\n",
    "・**ラッソ回帰**  \n",
    "ラッソ回帰はリッジ回帰と同様に、係数が小さくなるように回帰式を作成する手法である。リッジ回帰では正規化項として「係数の２乗の合計」を用いたが、ラッソ回帰では**「係数の絶対値の合計」を用いる。\n",
    "$$\n",
    "\\begin{align}\n",
    "E &= e_1^2 + e_2^2 + e_3^2 \\\\\n",
    "F &= |a| + |b| \\\\\n",
    "L &= E + F\n",
    "\\end{align}\n",
    "$$\n",
    "として、Lが最小になるような係数aとbを算出する。  \n",
    "ラッソ回帰は「不要な特徴量を削除した上で回帰式を作成するモデル」である。 予測を行う上で不要と判断された列の係数が0になる。   \n",
    "\n",
    "・**ラッソ回帰の実装**  \n",
    "alphaの値が大きいほど、正規化項を重要視しようとする。\n",
    "```python\n",
    "変数 = Lasso(alpha = 数値)\n",
    "```\n",
    "\n",
    "・**回帰木**  \n",
    "予測するためのフローチャートを作成できる決定木分析は分類だけでなく、回帰の問題でも利用することができる。回帰のためのフローチャートを**回帰木**、分類のためのフローチャートを**分類木**と呼び、両者まとめて「決定木」と呼ぶ。  \n",
    "回帰木では正解データが数値データとなる。この場合、正解データの平均値を予測値として出力するようなモデルとなる。  \n",
    "\n",
    "\n",
    " -----------------------------\n",
    "## まとめ  \n",
    "\n",
    "・回帰の手法であるリッジ回帰は、係数が小さくなるような回帰式を作成する。これにより、通常の重回帰に比べ過学習を抑止できる。  \n",
    "\n",
    "・回帰の手法であるラッソ回帰は、リッジ回帰と同様に係数が小さくなるような回帰式を作る。リッジ回帰とは正規化項の計算が異なり、不要な特徴量の係数を0にする。  \n",
    "\n",
    "・未知データに対する予測誤差は、バイアス・バリアンス・ノイズの三つに分解できる。  \n",
    "\n",
    "・過学習とは、「モデルを必要以上に複雑にした結果、バリアンスが異常に高くなった状態のこと」である。  \n",
    "\n",
    "・バリアンスとは予測結果の分散のこと。  \n",
    "\n",
    "・バイアスは、予測結果の平均値と正解データの平均値の誤差のこと。  \n",
    "\n",
    "・ノイズは正解データの分散のこと。  \n",
    "\n",
    "・決定木は回帰でも利用でき、回帰木と呼ぶ。\n",
    "\n",
    "\n",
    "--------------------------------------\n",
    "## 以下は実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d258478-0426-49a5-89b2-b94b7e8e8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('chap11/Boston.csv')\n",
    "df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "df = df.drop([76], axis = 0)\n",
    "\n",
    "t = df[['PRICE']]\n",
    "x = df.loc[:,['RM', 'PTRATIO','LSTAT']]\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc_x = sc.fit_transform(x)\n",
    "sc2 = StandardScaler()\n",
    "sc_t = sc2.fit_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aee6743e-7ddf-43e1-a9fb-4580e631f5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "pf_x = pf.fit_transform(sc_x) # 二乗列と交互作用特徴量の追加\n",
    "pf_x.shape # 行数と列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc1cab9-7bd1-48f6-b5ed-d83a347f3036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2',\n",
       "       'x2^2'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b527c8c2-373d-4329-8c9f-aa2e9d0c53b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2420892002.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(ridgeModel.score(x_test, y_test)):\u001b[39m\n                                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "x_train,x_test, y_train, y_test = train_test_split(pf_x, sc_t, test_size = 0.3, random_state = 0)\n",
    "from sklearn.linear_model import Ridge\n",
    "ridgeModel = Ridge(alpha = 10)\n",
    "ridgeModel.fit(x_train, y_train)\n",
    "print(ridgeModel.score(x_train, y_train))\n",
    "print(ridgeModel.score(x_test, y_test)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a7426-6071-4dce-bb27-3a7032ceb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxScore = 0\n",
    "maxIndex = 0\n",
    "\n",
    "for i in range(1, 2001):\n",
    "    num = i/100\n",
    "    ridgeModel = Ridge(random_state = 0, alpha = num)\n",
    "    ridgeModel.fit(x_train, y_train)\n",
    "\n",
    "    result = ridgeModel.score(x_test, y_test)\n",
    "    if result > maxScore:\n",
    "        maxScore = result\n",
    "        maxIndex = num\n",
    "\n",
    "print(maxIndex, maxScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8b9962-76cf-4dd9-94f0-0f8d2e59a8d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mabs\u001b[39m(\u001b[43mmodel\u001b[49m.coef_)[\u001b[32m0\u001b[39m])) \u001b[38;5;66;03m# 線形回帰の係数の合計\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mabs\u001b[39m(ridgeModel.coef_)[\u001b[32m0\u001b[39m]))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(sum(abs(model.coef_)[0])) # 線形回帰の係数の合計\n",
    "\n",
    "print(sum(abs(ridgeModel.coef_)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "626fe05d-f5a2-4e82-bdec-fb63783fc144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8224680202036665\n",
      "0.858846785318774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "x_train, x_test, y_train, y_test = train_test_split(pf_x, sc_t, test_size = 0.3, random_state = 0)\n",
    "model = Lasso(alpha = 0.1)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train))\n",
    "print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f564627-cae3-4dc6-ac04-fc02e855225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0       0.409426\n",
       "x1      -0.083104\n",
       "x2      -0.287714\n",
       "x0^2     0.150001\n",
       "x0 x1   -0.000000\n",
       "x0 x2   -0.037450\n",
       "x1^2    -0.000000\n",
       "x1 x2    0.000000\n",
       "x2^2     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = model.coef_\n",
    "pd.Series(weight, index = pf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "684da28b-2ffe-4c42-9a07-5899dfbccdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記の結果よりx0x1列、x1^2列、x1x2列、x2^2列の4列が不要と判断されていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26bf575b-9d75-4169-b1e1-defb167122fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('chap11/Boston.csv')\n",
    "df = df.fillna(df.mean(numeric_only=True))\n",
    "#df = df.drop([76], axis = 0) # 外れ値の行を削除\n",
    "x = df.loc[:, 'ZN':'LSTAT']\n",
    "t = df['PRICE']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, t,\n",
    "    test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ed578dd-1265-48b6-886b-51cff16c97b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59433275545417"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ライブラリインポート(回帰木バージョン)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 木の深さの最大を10と設定\n",
    "model = DecisionTreeRegressor(max_depth = 10,\n",
    "random_state = 0)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test, y_test) # テストデータでの決定係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b36357b-a566-40c3-ab86-33bc6aaf5129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZN         0.000252\n",
       "INDUS      0.007301\n",
       "CHAS       0.000000\n",
       "NOX        0.001967\n",
       "RM         0.759547\n",
       "AGE        0.139388\n",
       "DIS        0.013635\n",
       "RAD        0.000404\n",
       "TAX        0.013975\n",
       "PTRATIO    0.001913\n",
       "B          0.003331\n",
       "LSTAT      0.058287\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.feature_importances_, index = x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa16673-b906-49d3-a1c0-b7ab5ac7cdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
