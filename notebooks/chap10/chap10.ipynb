{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dteday\\tholiday\\tweekday\\tworkingday\\tweather_id\\tcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01\\t0\\t6\\t0\\t2\\t985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02\\t0\\t0\\t0\\t2\\t801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03\\t0\\t1\\t1\\t1\\t1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-04\\t0\\t2\\t1\\t1\\t1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-05\\t0\\t3\\t1\\t1\\t1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dteday\\tholiday\\tweekday\\tworkingday\\tweather_id\\tcnt\n",
       "0                        2011-01-01\\t0\\t6\\t0\\t2\\t985   \n",
       "1                        2011-01-02\\t0\\t0\\t0\\t2\\t801   \n",
       "2                       2011-01-03\\t0\\t1\\t1\\t1\\t1349   \n",
       "3                       2011-01-04\\t0\\t2\\t1\\t1\\t1562   \n",
       "4                       2011-01-05\\t0\\t3\\t1\\t1\\t1600   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('bike.tsv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dteday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather_id</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dteday  holiday  weekday  workingday  weather_id   cnt\n",
       "0  2011-01-01        0        6           0           2   985\n",
       "1  2011-01-02        0        0           0           2   801\n",
       "2  2011-01-03        0        1           1           1  1349\n",
       "3  2011-01-04        0        2           1           1  1562\n",
       "4  2011-01-05        0        3           1           1  1600"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bike.tsv',sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x90 in position 21: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df2=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweather.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\codecs.py:322\u001b[39m, in \u001b[36mBufferedIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    320\u001b[39m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[32m    321\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.buffer + \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     (result, consumed) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer = data[consumed:]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x90 in position 21: invalid start byte"
     ]
    }
   ],
   "source": [
    "df2=pd.read_csv(\"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"weather.csv\",encoding=\"shift-jis\")\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= pd.read_json(\"temp.json\")\n",
    "temp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.merge(weather,how=\"inner\",on=\"weather_id\")\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby(\"weather\").mean()['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=temp.T\n",
    "temp.loc[199:201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['dteday']=='2011-07-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.merge(temp,how=\"left\",on=\"dteday\")\n",
    "df3[df3[\"dteday\"]=='2011-07-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df3[\"temp\"].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[[\"temp\",\"hum\"]].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"temp\"].plot(kind=\"hist\")\n",
    "df3[\"hum\"].plot(kind=\"hist\",alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['atemp'].loc[220:240].plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atemp列のdtypeをfloatに変換\n",
    "df3[\"atemp\"] = df3[\"atemp\"].astype(float)\n",
    "\n",
    "\n",
    "df3[\"atemp\"] =df3[\"atemp\"].interpolate()\n",
    "df3.loc[225:235,\"atemp\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('iris.csv')\n",
    "non_df = iris_df.dropna() # 欠損値を含む行を削除\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "x = non_df.loc[:,\"がく片幅\":\"花弁幅\" ]\n",
    "t = non_df['がく片長さ']\n",
    "model = LinearRegression()\n",
    "model.fit(x,t) # 欠損値予測のためのモデルを予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損行の抜き出し\n",
    "condition = iris_df['がく片長さ'].isnull()\n",
    "non_data = iris_df.loc[ condition ]\n",
    "\n",
    "\n",
    "# 欠損行の入力に利用する特徴量だけを抜き出して、モデルで予測\n",
    "x = non_data.loc[:,\"がく片幅\":\"花弁幅\"]\n",
    "pred = model.predict(x)\n",
    "\n",
    "\n",
    "# 欠損行のがく片長さ(cm)のマスを抜き出して、predで代入\n",
    "iris_df.loc[condition,'がく片長さ']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import MinCovDet\n",
    "\n",
    "#数値列を適当に取り出す\n",
    "df4=df3.loc[:,\"atemp\":\"windspeed\"]\n",
    "df4=df4.dropna()#欠損値を削除\n",
    "\n",
    "#df4に対して、各データの中心点からのマハラノビス距離を計算\n",
    "\n",
    "mcd = MinCovDet(random_state=0,support_fraction=0.7)\n",
    "mcd.fit(df4)\n",
    "#マハラノビス距離\n",
    "distance = mcd.mahalanobis(df4)\n",
    "distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance=pd.Series(distance)\n",
    "distance.plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=distance.describe()#様々な基本統計量を計算\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = tmp['75%'] -tmp['25%']#IQR計算\n",
    "jougen = 1.5*(IQR) + tmp['75%'] # 上限値\n",
    "kagen = tmp['25%'] -1.5*(IQR) # 下限値\n",
    "\n",
    "# 上限と下限の条件をもとに、シリーズで条件検索\n",
    "outliner = distance[ (distance > jougen) | (distance < kagen) ]\n",
    "outliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず、ダミー変数化をしたいが、文字列の列が複数あるので抜き出す。\n",
    "str_col_name=['job','default','marital','education','housing','loan','contact','month']\n",
    "str_df = df[str_col_name]\n",
    "#複数列を一気にダミー変数化\n",
    "str_df2=pd.get_dummies(str_df,drop_first=True)\n",
    "\n",
    "num_df = df.drop(str_col_name,axis=1)#数値列を抜き出す\n",
    "df2 = pd.concat([num_df,str_df2,str_df],axis=1)#結合(今後の集計の利便性も考慮してstr_dfも結合しておく)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練&検証データとテストデータに分割\n",
    "train_val,test = train_test_split(df2,test_size=0.1,random_state=9)\n",
    "train_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠損値の確認\n",
    "is_nan=train_val.isnull().sum()\n",
    "#欠損が存在している列だけ表示\n",
    "is_nan[is_nan>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改善案1  欠損値の補完方法を線形回帰で行ってみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val.corr()['duration'].map(abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量の当たりがついた\n",
    "#しかし、そもそもこの線形回帰は外れ値の影響を強く受けるので調べる。\n",
    "num_df=train_val.drop(str_col_name,axis=1)\n",
    "num_df=num_df.drop('id',axis=1)\n",
    "num_df2=num_df.dropna()\n",
    "mcd2 =MinCovDet(random_state=0,support_fraction=0.7)\n",
    "mcd2.fit(num_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis =mcd2.mahalanobis(num_df2)\n",
    "dis=pd.Series(dis)\n",
    "dis.plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dis[0:3])#先頭は0番からのラベル\n",
    "no=dis[dis>300000].index\n",
    "no[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先頭から2561番目が外れ値となる事が分かったので９章の付録で紹介したilocを利用する\n",
    "no=num_df2.iloc[no[0]:(no[0]+1),:].index\n",
    "train_val2 = train_val.drop(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val2.corr()['duration'].map(abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthはdurationに本質的に影響あるとは思えないので特徴量を\n",
    "#housing_yes ,loan_yes,age,marital_single ,job_student    とする。（ｙは最終的な正解データなので除外）    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠損行を削除\n",
    "not_nan_df = train_val2.dropna()\n",
    "temp_t =not_nan_df['duration']\n",
    "temp_x = not_nan_df[['housing_yes','loan_yes','age','marital_single' ,'job_student']]\n",
    "# 線形回帰\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_liner = LinearRegression()\n",
    "\n",
    "a,b,c,d= train_test_split(temp_x,temp_t,random_state=0,test_size=0.2)\n",
    "\n",
    "#今回は予測させたいだけなので、標準化はしない\n",
    "model_liner.fit(a,c)\n",
    "print(model_liner.score(a,c),model_liner.score(b,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コード修正(不要なので削除)\n",
    "# tain_val2 = train_val.copy()\n",
    "\n",
    "is_null=train_val2['duration'].isnull()\n",
    "non_x=train_val2.loc[is_null,['housing_yes','loan_yes','age','marital_single','job_student']]\n",
    "pred_d = model_liner.predict(non_x)\n",
    "train_val2.loc[is_null,'duration']=pred_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ヒストグラムの確認\n",
    "train_val2.loc[train_val['y']==0,\"duration\"].plot(kind=\"hist\")\n",
    "train_val2.loc[train_val['y']==1,\"duration\"].plot(kind=\"hist\",alpha=0.4)\n",
    "\n",
    "#y=1の方が、durationが大きい傾向がやっぱりありそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#まず、さくっと学習できるようなlearn関数を定義する。\n",
    "def learn(x,t,i):\n",
    "    x_train,x_val,y_train,y_val = train_test_split(x,t,test_size=0.2,random_state=13)\n",
    "\n",
    "    datas=[x_train,x_val,y_train,y_val]\n",
    "    #不均衡データに対応できるように、class_weight引数も設定\n",
    "    model = tree.DecisionTreeClassifier(random_state=i,max_depth=i,class_weight='balanced')\n",
    "    model.fit(x_train,y_train)\n",
    "    train_score=model.score(x_train,y_train)\n",
    "    \n",
    "    \n",
    "    val_score=model.score(x_val,y_val)\n",
    "    return train_score,val_score,model,datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t =train_val2['y']\n",
    "x = train_val2.drop(str_col_name,axis=1)\n",
    "x =x.drop(['id','y','day'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#とりあえず、for文で様々な木の深さでの正解率を調べてみる\n",
    "for i in range(1,15):\n",
    "    s1,s2,model,datas = learn(x,t,i)\n",
    "    print(i,s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テストデータでも調べる\n",
    "test2 = test.copy()\n",
    "isnull=test2['duration'].isnull()\n",
    "model_tree=tree.DecisionTreeClassifier(random_state=10,max_depth=10,class_weight=\"balanced\")\n",
    "if isnull.sum()>0:\n",
    "    temp_x=test2.loc[isnull,['housing_yes','loan_yes','age','marital_single','job_student']]\n",
    "    pred_d = model_liner.predict(temp_x)\n",
    "    test2.loc[isnull,'duration']=pred_d\n",
    "x_test = test2.drop(str_col_name,axis=1)\n",
    "x_test =x_test.drop(['id','y','day'],axis=1)\n",
    "y_test = test['y']\n",
    "\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9章の最後より若干低下している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#どのような間違い方をしているのか確認\n",
    "s1,s2,model,datas = learn(x,t,9)\n",
    "\n",
    "#訓練データでの予測結果と実際の値の2軸で個数集計flagがFalseならば、検証データで集計\n",
    "def syuukei(model,datas,flag=False):\n",
    "    if flag:\n",
    "        pre=model.predict(datas[0])\n",
    "        y_val=datas[2]\n",
    "    else:\n",
    "        pre=model.predict(datas[1])\n",
    "        y_val=datas[3]\n",
    "    data={\n",
    "        \"pred\":pre,\n",
    "        \"true\":y_val\n",
    "    }\n",
    "    tmp=pd.DataFrame(data)\n",
    "    return tmp,pd.pivot_table(tmp,index=\"true\",columns=\"pred\",values=\"true\",aggfunc=len)\n",
    "tmp,a=syuukei(model,datas,False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練データと検証データの間違い型の傾向を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#値にばらつきが大きいので、標準化してもう一度グラフ化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "tmp2=train_val2.drop(str_col_name,axis=1)\n",
    "sc_data = sc.fit_transform(tmp2)\n",
    "sc_df = pd.DataFrame(sc_data,columns=tmp2.columns,index=tmp2.index)\n",
    "\n",
    "######挿入箇所#######\n",
    "pre = model.predict(sc_df.drop([\"id\",\"day\",\"y\"],axis=1))\n",
    "target = tmp2[\"y\"]\n",
    "true = (pre == target)\n",
    "false = (pre!= target)\n",
    "############\n",
    "\n",
    "true_df=sc_df.loc[true]\n",
    "false_df=sc_df.loc[false]\n",
    "true_df\n",
    "temp2=pd.concat([false_df.mean()[\"age\":],true_df.mean()[\"age\":]],axis=1)\n",
    "temp2.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_val2.groupby('loan')['y'].mean())\n",
    "print(train_val2.groupby('housing')['y'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val3=train_val2.copy()\n",
    "train_val3['du*hou']=train_val3['duration']*train_val3['housing_yes']\n",
    "train_val3['du*loan']=train_val3['duration']*train_val3['loan_yes']\n",
    "train_val3['du*age']=train_val3['duration']*train_val3['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t =train_val3['y']\n",
    "\n",
    "monthcol=['month_aug',\n",
    "       'month_dec', 'month_feb', 'month_jan', 'month_jul', 'month_jun',\n",
    "       'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep']\n",
    "jobcol=['job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired',\n",
    "       'job_self-employed', 'job_services', 'job_student', 'job_technician',\n",
    "       'job_unemployed', 'job_unknown']\n",
    "x = train_val3.drop(str_col_name,axis=1)\n",
    "x = x.drop(jobcol,axis=1)\n",
    "\n",
    "x = x.drop(monthcol,axis=1)\n",
    "x =x.drop(['id','y','day'],axis=1)\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#とりあえず、for文で様々な木の深さでの正解率を調べてみる\n",
    "for i in range(5,15):\n",
    "    s1,s2,model,datas = learn(x,t,i)\n",
    "    print(i,s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1,s2,model,datas = learn(x,t,9)\n",
    "tmp,a=syuukei(model,datas,False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(model.feature_importances_,index=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=9\n",
    "model = tree.DecisionTreeClassifier(random_state=i,max_depth=i,class_weight=\"balanced\")\n",
    "model.fit(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テストデータでも調べる\n",
    "test2 = test.copy()\n",
    "isnull=test['duration'].isnull()\n",
    "if isnull.sum()>0:\n",
    "    temp_x=test2.loc[isnull,['housing_yes','loan_yes','age','marital_single','job_student']]\n",
    "    pred_d = model_liner.predict(temp_x)\n",
    "    test2.loc[isnull,'duration']=pred_d\n",
    "\n",
    "test2['du*hou']=test2['duration']*test2['housing_yes']\n",
    "test2['du*loan']=test2['duration']*test2['loan_yes']\n",
    "test2['du*age']=test2['duration']*test2['age']\n",
    "\n",
    "x_test = test2.drop(str_col_name,axis=1)\n",
    "x_test = x_test.drop(jobcol,axis=1)\n",
    "x_test = x_test.drop(monthcol,axis=1)\n",
    "x_test =x_test.drop(['id','y','day'],axis=1)\n",
    "y_test = test['y']\n",
    "x_test.columns\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直観的に考えて、9章では、housingとloanで集計しており、今回の線形回帰では、それらの列も含まれているから、\n",
    "# durationの性能はよりよくなるはず、でも全体のモデルの正解率は1%ほど低下している\n",
    "\n",
    "#原因の仮説⇒ １．現状の線形回帰だと訓練&検証に過学習してしまい、テストデータにフィットしない。\n",
    "                 #（そもそもテストデータではdurationがあまり関係していない？？）\n",
    "#            2. 純粋な決定木の限界？\n",
    "\n",
    "#            3. 現在考慮していない特徴量ももっとしっかりした方が良いのか？？\n",
    "\n",
    "#次以降の章で仮説1,2について検討できるので、次章に続く。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
