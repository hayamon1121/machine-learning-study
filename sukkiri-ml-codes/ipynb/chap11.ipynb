{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 絶対使うであろうモジュールのインポート\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Boston.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBoston.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# csvの読み込み\u001b[39;00m\n\u001b[32m      2\u001b[39m df = df.fillna(df.mean()) \u001b[38;5;66;03m# 欠損値補完\u001b[39;00m\n\u001b[32m      3\u001b[39m df = df.drop([\u001b[32m76\u001b[39m], axis = \u001b[32m0\u001b[39m) \u001b[38;5;66;03m# 外れ値の行を削除\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Boston.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Boston.csv') # csvの読み込み\n",
    "df = df.fillna(df.mean()) # 欠損値補完\n",
    "df = df.drop([76], axis = 0) # 外れ値の行を削除\n",
    "\n",
    "t = df[['PRICE']] # 正解データ抜き出し\n",
    "x = df.loc[:,['RM', 'PTRATIO', 'LSTAT']] # 特徴量抜き出し\n",
    "\n",
    "# 標準化\n",
    "sc = StandardScaler()\n",
    "sc_x = sc.fit_transform(x)\n",
    "sc2 = StandardScaler()\n",
    "sc_t = sc2.fit_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "pf_x = pf.fit_transform(sc_x) # 2乗列と交互作用項の追加\n",
    "pf_x.shape # 行数と列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(pf_x,\n",
    "    sc_t, test_size = 0.3, random_state = 0)\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train)) # 訓練データの決定係数\n",
    "model.score(x_test, y_test) # テストデータの決定係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge # モジュールインポート\n",
    "# モデルの作成\n",
    "ridgeModel = Ridge(alpha = 10)\n",
    "ridgeModel.fit(x_train, y_train) # 学習\n",
    "print(ridgeModel.score(x_train, y_train))\n",
    "print(ridgeModel.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxScore = 0\n",
    "maxIndex = 0\n",
    "# range関数により整数列を1～2000生成\n",
    "for i in range(1, 2001):\n",
    "    num = i/100\n",
    "    ridgeModel = Ridge(random_state = 0, alpha = num)\n",
    "    ridgeModel.fit(x_train, y_train)\n",
    "    result = ridgeModel.score(x_test, y_test)\n",
    "    if result > maxScore:\n",
    "        maxScore = result\n",
    "        maxIndex = num\n",
    "\n",
    "print(maxIndex, maxScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(abs(model.coef_)[0])) # 線形回帰の係数（絶対値）\n",
    "# の合計\n",
    "print(sum(abs(ridgeModel.coef_)[0])) # リッジ回帰の合計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(pf_x,\n",
    "    sc_t, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# ラッソ回帰のモデル作成（alphaは正則化項につく定数）\n",
    "model = Lasso(alpha = 0.1)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_train, y_train)) # 訓練データの決定係数\n",
    "print(model.score(x_test, y_test)) # テストデータの決定係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.coef_ # 係数抜き出す\n",
    "# 見やすいようにシリーズ変換\n",
    "pd.Series(weight, index = pf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('Boston.csv')\n",
    "df = df.fillna(df.mean())\n",
    "#df = df.drop([76], axis = 0) # 外れ値の行を削除\n",
    "x = df.loc[:, 'ZN':'LSTAT']\n",
    "t = df['PRICE']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, t,\n",
    "    test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリインポート(回帰木バージョン)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 木の深さの最大を10と設定\n",
    "model = DecisionTreeRegressor(max_depth = 10,\n",
    "random_state = 0)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test, y_test) # テストデータでの決定係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series( model.feature_importances_, index = x.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#練習問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず、ダミー変数化をしたいが、文字列の列が複数あるので抜き出す。\n",
    "str_col_name=['job','default','marital','education','housing','loan','contact','month']\n",
    "str_df = df[str_col_name]\n",
    "#複数列を一気にダミー変数化\n",
    "str_df2=pd.get_dummies(str_df,drop_first=True)\n",
    "\n",
    "num_df = df.drop(str_col_name,axis=1)#数値列を抜き出す\n",
    "df2 = pd.concat([num_df,str_df2,str_df],axis=1)#結合(今後の集計の利便性も考慮してstr_dfも結合しておく)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練&検証データとテストデータに分割\n",
    "train_val,test = train_test_split(df2,test_size=0.1,random_state=9)\n",
    "train_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量の当たりがついた\n",
    "#しかし、そもそもこの線形回帰は外れ値の影響を強く受けるので調べる。\n",
    "from sklearn.covariance import MinCovDet\n",
    "num_df=train_val.drop(str_col_name,axis=1)\n",
    "num_df=num_df.drop('id',axis=1)\n",
    "num_df2=num_df.dropna()\n",
    "mcd2 =MinCovDet(random_state=0,support_fraction=0.7)\n",
    "mcd2.fit(num_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis =mcd2.mahalanobis(num_df2)\n",
    "dis=pd.Series(dis)\n",
    "dis.plot(kind=\"box\")\n",
    "no=dis[dis>300000].index\n",
    "#先頭から2561番目が外れ値となる事が分かったので９章の付録で紹介したilocを利用する\n",
    "no=num_df2.iloc[no[0]:(no[0]+1),:].index\n",
    "train_val2 = train_val.drop(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠損行を削除\n",
    "not_nan_df = train_val2.dropna()\n",
    "temp_t =not_nan_df[['duration']]\n",
    "temp_x = not_nan_df.drop(str_col_name,axis=1)\n",
    "\n",
    "#durationとyに関係があるという仮定が成り立つならば、適切な推定をするためには,\n",
    "temp_x = temp_x.drop(['y','duration','id'],axis=1)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "\n",
    "a,b,c,d= train_test_split(temp_x,temp_t,random_state=0,test_size=0.2)\n",
    "maxvalue=0\n",
    "v=0\n",
    "#今回はLasso回帰を利用するので、特徴量選択はしない\n",
    "for i in range(1,42):\n",
    "    val = i/20\n",
    "    model_liner = Lasso(random_state=0,alpha=val)\n",
    "    #今回は予測させたいだけなので、標準化はしない\n",
    "    model_liner.fit(a,c)\n",
    "    if maxvalue < model_liner.score(b,d):\n",
    "        v=val\n",
    "        maxvalue = model_liner.score(b,d)\n",
    "print(v,maxvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_liner = Lasso(random_state=0,alpha=v)\n",
    "#今回は予測させたいだけなので、標準化はしない\n",
    "model_liner.fit(a,c)\n",
    "#pd.Series(model_liner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 考え方をここで変える。durationとyに関係が強いという仮定が正しいならば、durationを推定するのに\n",
    "#yを利用するのは合理的ではなかろうか？ただテストデータでは、yの値が本当に未知という状況で検証するので\n",
    "#テストデータでもdurationが欠損している場合は上記model_linerを利用する。\n",
    "#欠損行を削除\n",
    "not_nan_df = train_val2.dropna()\n",
    "temp_t =not_nan_df[['duration']]\n",
    "temp_x = not_nan_df.drop(str_col_name,axis=1)\n",
    "#yを消さない\n",
    "temp_x = temp_x.drop(['duration','id'],axis=1)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "\n",
    "a,b,c,d= train_test_split(temp_x,temp_t,random_state=0,test_size=0.2)\n",
    "maxvalue=0\n",
    "v=0\n",
    "for i in range(1,42):\n",
    "    val = i/20\n",
    "    model_liner2 = Lasso(random_state=0,alpha=val)\n",
    "    #今回は予測させたいだけなので、標準化はしない\n",
    "    model_liner2.fit(a,c)\n",
    "    if maxvalue < model_liner2.score(b,d):\n",
    "        v=val\n",
    "        maxvalue = model_liner2.score(b,d)\n",
    "print(v,maxvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果\n",
    "pd.Series(model_liner2.coef_,index=temp_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val3 = train_val.copy()\n",
    "is_null=train_val3['duration'].isnull()\n",
    "#temp_x = tain_val3.drop(str_col_name,axis=1)\n",
    "#修正\n",
    "temp_x = train_val3.drop(str_col_name,axis=1)\n",
    "\n",
    "temp_x = temp_x.drop(['duration','id'],axis=1)\n",
    "temp_x = temp_x[is_null]\n",
    "#non_x=train_val2.loc[is_null,['housing_yes','loan_yes','age','marital_single','job_student']]\n",
    "pred_d = model_liner2.predict(temp_x)\n",
    "train_val3.loc[is_null,'duration']=pred_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ヒストグラムの確認\n",
    "train_val3.loc[train_val3['y']==0,\"duration\"].plot(kind=\"hist\")\n",
    "train_val3.loc[train_val3['y']==1,\"duration\"].plot(kind=\"hist\",alpha=0.4)\n",
    "\n",
    "#y=1の方が、durationが大きい傾向がやっぱりありそう\n",
    "train_val3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val3[\"duration\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#まず、さくっと学習できるようなlearn関数を定義する。\n",
    "def learn(x,t,i):\n",
    "    x_train,x_val,y_train,y_val = train_test_split(x,t,test_size=0.2,random_state=13)\n",
    "\n",
    "    datas=[x_train,x_val,y_train,y_val]\n",
    "    #不均衡データに対応できるように、class_weight引数も設定\n",
    "    model = tree.DecisionTreeClassifier(random_state=i,max_depth=i,class_weight='balanced')\n",
    "    model.fit(x_train,y_train)\n",
    "    train_score=model.score(x_train,y_train)\n",
    "    \n",
    "    \n",
    "    val_score=model.score(x_val,y_val)\n",
    "    return train_score,val_score,model,datas\n",
    "\n",
    "t =train_val3['y']\n",
    "x = train_val3.drop(str_col_name,axis=1)\n",
    "x =x.drop(['id','y','day'],axis=1)\n",
    "#とりあえず、for文で様々な木の深さでの正解率を調べてみる\n",
    "for i in range(1,15):\n",
    "    s1,s2,model,datas = learn(x,t,i)\n",
    "    print(i,s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深さ8 検証データの正解率が0.82 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#どのような間違い方をしているのか確認\n",
    "s1,s2,model,datas = learn(x,t,8)\n",
    "\n",
    "#訓練データでの予測結果と実際の値の2軸で個数集計flagがFalseならば、検証データで集計\n",
    "def syuukei(model,datas,flag=False):\n",
    "    if flag:\n",
    "        pre=model.predict(datas[0])\n",
    "        y_val=datas[2]\n",
    "    else:\n",
    "        pre=model.predict(datas[1])\n",
    "        y_val=datas[3]\n",
    "    data={\n",
    "        \"pred\":pre,\n",
    "        \"true\":y_val\n",
    "    }\n",
    "    tmp=pd.DataFrame(data)\n",
    "    return tmp,pd.pivot_table(tmp,index=\"true\",columns=\"pred\",values=\"true\",aggfunc=len)\n",
    "tmp,a=syuukei(model,datas,False)\n",
    "print(a)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "false=tmp.loc[(tmp['pred']==1)&(tmp['true']==0)].index\n",
    "true=tmp.loc[(tmp['pred']==0)&(tmp['true']==0)].index\n",
    "true_df=train_val3.loc[true]\n",
    "false_df=train_val3.loc[false]\n",
    "sc = StandardScaler()\n",
    "tmp2=train_val3.drop(str_col_name,axis=1)\n",
    "sc_data = sc.fit_transform(tmp2)\n",
    "sc_df = pd.DataFrame(sc_data,columns=tmp2.columns,index=tmp2.index)\n",
    "\n",
    "true_df=sc_df.loc[true]\n",
    "false_df=sc_df.loc[false]\n",
    "true_df\n",
    "temp2=pd.concat([false_df.mean()[\"age\":],true_df.mean()[\"age\":]],axis=1)\n",
    "temp2.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交互作用項を付けてみる\n",
    "train_val4=train_val3.copy()\n",
    "train_val4['du*hou']=train_val3['duration']*train_val3['housing_yes']\n",
    "train_val4['du*loan']=train_val3['duration']*train_val3['loan_yes']\n",
    "train_val4['du*age']=train_val3['duration']*train_val3['age']\n",
    "\n",
    "t =train_val4['y']\n",
    "\"\"\"\n",
    "monthcol=['month_aug',\n",
    "       'month_dec', 'month_feb', 'month_jan', 'month_jul', 'month_jun',\n",
    "       'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep']\n",
    "#jobcol=['job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired',\n",
    "       'job_self-employed', 'job_services', 'job_student', 'job_technician',\n",
    "       'job_unemployed', 'job_unknown']\n",
    "\"\"\"\n",
    "\n",
    "x = train_val4.drop(str_col_name,axis=1)\n",
    "\n",
    "#x = x.drop(jobcol,axis=1)\n",
    "\n",
    "#x = x.drop(monthcol,axis=1)\n",
    "x =x.drop(['id','y','day'],axis=1)\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#とりあえず、for文で様々な木の深さでの正解率を調べてみる\n",
    "for i in range(5,15):\n",
    "    s1,s2,model,datas = learn(x,t,i)\n",
    "    print(i,s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#深さ８で正解率81% 先ほどより低下した。よって交互作用項は取る。\n",
    "train_val4=train_val3.copy()\n",
    "t =train_val4['y']\n",
    "x = train_val4.drop(str_col_name,axis=1)\n",
    "x =x.drop(['id','y','day'],axis=1)\n",
    "i=8\n",
    "model = tree.DecisionTreeClassifier(random_state=i,max_depth=i,class_weight=\"balanced\")\n",
    "model.fit(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テストデータでも調べる\n",
    "test2 = test.copy()    \n",
    "isnull=test2['duration'].isnull()\n",
    "print(isnull.shape)\n",
    "if isnull.sum()>0:\n",
    "    temp_x = test2.drop(str_col_name,axis=1)\n",
    "    temp_x = temp_x.drop(['y','duration','id'],axis=1)\n",
    "    #print(temp_x.shape[0])\n",
    "    temp_x = temp_x[isnull]\n",
    "    #ここではmodel_linerで調べる\n",
    "    pred_d = model_liner.predict(temp_x)\n",
    "    test2.loc[isnull,'duration']=pred_d\n",
    "    \n",
    "\n",
    "x_test = test2.drop(str_col_name,axis=1)\n",
    "x_test =x_test.drop(['id','y','day'],axis=1)\n",
    "y_test = test['y']\n",
    "x_test.columns\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#もしかしたら、テストデータにも結構durationの欠損値があるのかもしれない（テストデータなので確認できない）\n",
    "#よってmodel_linerによる不適切なduration推定をしているかもしれない。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10章で仮説を3つ挙げた\n",
    "#精度が上がりづらい原因の仮説⇒ \n",
    "              #１．現状の線形回帰だと訓練&検証に過学習してしまい、テストデータにフィットしない。\n",
    "                 #（そもそもテストデータではdurationがあまり関係していない？？）\n",
    "#            2. 純粋な決定木の限界？\n",
    "\n",
    "#            3. 現在考慮していない特徴量ももっとしっかりした方が良いのか？？\n",
    "\n",
    "# 考察\n",
    "#今回、過学習しづらいlasso回帰を利用したので、10章よりかは１の可能性が減るはずだが、性能はあまり変わらない。\n",
    "#現状の知識では、1の可能性は低い。よって２か３の可能性を次章以降で探る（ただし、２と１の組み合わせなどは可能性\n",
    "#としてあることに注意）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
